{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d12b5e",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e404ec15",
   "metadata": {},
   "source": [
    "In this lab we will implement an application of the K-Means clustering algorithm, the first in our line of unsupervised methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3425a2",
   "metadata": {},
   "source": [
    "As a reiteration of the core concept, given a set of data points X, meant to be clustered into _k_ groups, the algorithm aims at determining $k$ cluster centers. Subsequently, each point is going to be assinged to a cluster based on the shortest distance towards a cluster center. \n",
    "\n",
    "Let $c_i \\in \\mathbf{R}^n$ denote the center of cluster $i = \\overline{1,k}$.\n",
    "\n",
    "Each data point $x_i \\in X$ is going to be assigned to cluster $argmin_j d(x_i, c_j)$, where d is the chosen distance function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b561238",
   "metadata": {},
   "source": [
    "When determining the cluster centers, we start by assigning $k$ random cluster centers within our data distribution. Subsequently, a series of two steps if performed repeatedly until convergence.\n",
    "\n",
    "1. Expectation - Based on the current centroids, each point is assigned to a cluster.\n",
    "2. Maximization - Based on the current cluster assignment, each centroid is moved to the center of the cluster it has determined.\n",
    "\n",
    "The following figure illustrates this process:\n",
    "\n",
    "![kmeans](./kmeans.png)\n",
    "\n",
    "The expectation and maximization steps are repeated for a set number of iterations or until the cluster centers have stabilized and no longer update via this procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e7056",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "For this lab we will use the sklearn implementation of the K-Means algorithm. The following is a short usage example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06494320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0]\n",
      "[[10.  2.]\n",
      " [ 1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# loading the data\n",
    "X = np.array([\n",
    "    [1, 2], [1, 4], [1, 0],\n",
    "    [10, 2], [10, 4], [10, 0]\n",
    "])\n",
    "\n",
    "# intantiating the model and fitting the data, similar to previous sklearn models\n",
    "kmeans = KMeans(n_clusters = 2, random_state = 0).fit(X)\n",
    "print(kmeans.labels_)\n",
    "# array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
    "\n",
    "# using the trained model to make predictions for new data\n",
    "kmeans.predict([[0, 0], [12, 3]])\n",
    "# array([1, 0], dtype=int32)\n",
    "\n",
    "print(kmeans.cluster_centers_)\n",
    "# array([[10.,  2.], [ 1.,  2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512af44",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b520e83",
   "metadata": {},
   "source": [
    "In this lab we will start implementing the pipepline of a large scale unsupervised classification procedure. This method has been proposed by Caron et al in the following ECCV 2018 paper: https://openaccess.thecvf.com/content_ECCV_2018/papers/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.pdf\n",
    "\n",
    "As described in the paper, the goal is to train a neural network to classify samples without the need for human annotations. In order to do this, the authors propose the exaptation of the expectation-maximization procedure.\n",
    "\n",
    "They start with a randomly initiallized neural network and perform the following two steps alternatively until convergence:\n",
    "\n",
    "1. Expectation - (i) the data is passed through the neural network in order to extract features, (ii) the samples are clustered using the k-means algorithm based on the extracted features\n",
    "2. Maximization - the network is trained to match the labels assigned by the k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee8f4d",
   "metadata": {},
   "source": [
    "We will divide this procedure into steps and implement those steps sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf88da",
   "metadata": {},
   "source": [
    "First we will load the data. For this task we will work with the MNist dataset. The data has been prepared in the mnist folder. In turn this folder contains 3 subfolders: train, val and test. Within each subfolder samples have a filename which follows the pattern {label}\\_{sample\\_number}.{extension}. That is, in order to determine the label of a given sample, one can simply split the filename based on the '\\_' character and convert the first token to int."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f0117",
   "metadata": {},
   "source": [
    "As a first task, write the code that reads the data. Complete the 'read_dataset' function, which, given a use case (train, val, test) reads the data from the appropriate folder and returns a numpy array of images and a numpy array of labels for those images. As a preprocessing step, we will normalize the images via division by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de436d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(use): # use can be 'train', 'val' or 'test'\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    return images, labels\n",
    "    \n",
    "train_images, train_labels = read_dataset('train')\n",
    "val_images, val_labels = read_dataset('val')\n",
    "test_images, test_labels = read_dataset('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2aae0",
   "metadata": {},
   "source": [
    "Next, we will define our neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed952f7",
   "metadata": {},
   "source": [
    "Define a convolutional neural network with the following architecture:\n",
    "\n",
    "- a convolutional layer with 64 filters, kernel size of 5, and relu activation\n",
    "- a max pooling layer with a size of 2\n",
    "- a convolutional layer with 64 filters, kernel size of 5, and relu activation\n",
    "- a max pooling layer with a size of 2\n",
    "- a flattening layer\n",
    "- a dense layer with a size of 512 and relu activation with the name 'features'. We name this layer (by specifying name = 'features' in the layer parameters) in order to be able to access its features. We will use those features to perform clustering\n",
    "- a dense layer with 10 neurons and softmax activation function for classification\n",
    "\n",
    "Compile the model using the Adam optimizer, a sparse categorical cross-entropy loss and accuracy as an evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f231e0",
   "metadata": {},
   "source": [
    "You can test your model the implementation of the model by training it for an epoch on the mnist dataset and noticing its performance on the test set (which should be above 95%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b85242",
   "metadata": {},
   "source": [
    "Next, we will define a K-Means model with 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f6073",
   "metadata": {},
   "source": [
    "Now that we have defined our prerequisites, we will start the actual procedure. For the expectation step, we will pass the data through the model, taking the output of the second to last layer, and cluster the samples based on those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a247a",
   "metadata": {},
   "source": [
    "We can get the output of the 'features' layer in tensorflow by defining a new \"partial model\" which takes inputs the same way the normal model does and outputs the result of the 'features' layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_model = tf.keras.models.Model(\n",
    "    inputs = model.input,\n",
    "    outputs = model.get_layer('features').output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7877cdd0",
   "metadata": {},
   "source": [
    "Now, we should pass all the training data through the partial model in order to get the features. Iterate through the data passing batches (32 or 64) of samples through the model and collecting the representations for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319398d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_features = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6cef0",
   "metadata": {},
   "source": [
    "Finally, fit the K-Means model on the collected features and get their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53c729",
   "metadata": {},
   "source": [
    "For the maximization step, train the neural network for an epoch on the kmeans_features and kmeans_labels. In the interest of time, for this exercise you can shorten the training by specifying steps_per_epoch = 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e578b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    # your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a506b44",
   "metadata": {},
   "source": [
    "After implementing the expectation and maximization procedures. It is time to formally evaluate our model. When doing this we should take into account the fact that, the clustering labels are most likely not alligned with the supervised labels. In order to do that, when computing the performance on the test set, we should be careful to match the labels predicted by the network with the test labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a25bc",
   "metadata": {},
   "source": [
    "Firstly, iterate through the test dataset in order to get the network predictions for each sample. Due to memory constraints, it is recommended to do this in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a454fbb",
   "metadata": {},
   "source": [
    "In order to compute the accuracy properly, we have to match the clustering labels with the real labels. We will implement a very straight forwards way of doing this. Computing the confusion matrix, we will assign each cluster to the label with which it has the most predictions in common. That is, a given cluster (predicted label) $i$, will be converted by replacing it with the position of the highest value from column $i$ from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = # your code here\n",
    "adjusted_predictions = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5012f74",
   "metadata": {},
   "source": [
    "Finally, evaluate the model by computing the accuracy, comparing the adjusted predictions with the test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = # your code here\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ee2ea",
   "metadata": {},
   "source": [
    "As a final task, you can continue the training by looping over the expectation and maximization steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
