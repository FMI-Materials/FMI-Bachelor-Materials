{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering - K-means, Hierarchical Clustering, DBSCAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will perform unsupervised classification using clustering algorithms. This will give you an opportunity to explore different clustering methods and different setups for those methods in order to get an intuition as to how the process of performing unsupervised classifiation looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First, we have our dataset. For this lab we will work with a few categories from the Reuters dataset. The code below will download and preprocess the dataset in order to allow us to spend more time on the actual techniques.\n",
    "\n",
    "First, download the dataset to the current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('reuters', download_dir = './')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, go to the corpora folder located at the current path and unzip the reuters archive. Subsequently, run the rest of the processing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = './corpora/reuters'\n",
    "with open(f'{dataset_folder}/cats.txt', 'r') as f:\n",
    "    annotations = f.readlines()\n",
    "    \n",
    "selected_categories = ['sugar', 'livestock', 'jobs', 'ship']\n",
    "category_to_index = dict(zip(selected_categories, range(len(selected_categories))))\n",
    "\n",
    "train_texts, train_labels, test_texts, test_labels = [], [], [], []\n",
    "for ann in annotations:\n",
    "    ann = ann.rstrip().split()\n",
    "    \n",
    "    if not any([category in ann for category in selected_categories]):\n",
    "        continue\n",
    "    \n",
    "    document_text = open(f'{dataset_folder}/{ann[0]}', 'r').read()\n",
    "    label = category_to_index[\n",
    "        [category for category in selected_categories if category in ann[1:]][0]\n",
    "    ]\n",
    "    \n",
    "    if 'train' in ann[0]:\n",
    "        train_texts.append(document_text)\n",
    "        train_labels.append(label)\n",
    "    else:\n",
    "        test_texts.append(document_text)\n",
    "        test_labels.append(label)\n",
    "    \n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this preprocessing we end up with 4 arrays.\n",
    "- train_texts - which is a list of all the texts in the train dataset\n",
    "- train_labels - which is a numpy array with the labels of train_texts, corresponding to the chosen categories\n",
    "- test_texts - which is a list of all the texts in the test dataset\n",
    "- test_labels - which is a numpy array with the labels of test_texts, corresponding to the chosen categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "For this exercise we will use a simple TF-IDF vectorizer from the sklearn library.\n",
    "\n",
    "Your first task is to compute the train_data and test_data variables, which should be the results of applying the TfidfVectorizer from the sklearn library on our dataset. Use a maximum of 500 features. Fit the vectorizer on the training texts and use it to transform both training and test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ...\n",
    "test_data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "\n",
    "To ensure that everything is okay with our data and preprocessing, and, in order to have a baseline as a reference, fit an SVC on the training data and evaluate it on the test split. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first clustering algorithm we will investigate is the one we've already seen: K-Means.\n",
    "\n",
    "As a basline, fit the a K-Means model, without any change in parameters on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline k-means model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Since we work in an unsupervised classification scenario, we will evaluate or model with respect to the class labels that we have. In order to do that, we have to match each cluster to a class (since clusters are not ordered in any particular order). For that, we will compute the confusion matrix (m\\[i\\]\\[j\\] = number of samples from class i assigned to cluster j) on the training set and use it do determine the best matching.\n",
    "\n",
    "In order to perform the matching we will use the linear_sum_assignment method implemented in the scipy.optimize package (https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.linear_sum_assignment.html). This method requires a cost function in order to compute the best matching. For that, we will feed the inverse of the confusion matrix, that is 1.0/confusion_matrix.\n",
    "\n",
    "Once you compute the best matching, translate the cluster labels into class labels and evaluate the accuracy of the model on the training set as well as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "\n",
    "Make changes such as the following and take note on how those influence the performance of the model.\n",
    "\n",
    "1. Consider the fact that the classes are unbalanced. In order to account for that in the cost matrix, divide each row of the confusion matrix by its sum before passing it to the linear_sum_assignment procedure.\n",
    "2. Insted of passing the inverse of the confusion_matrix as a cost, pass the negative of the confusion matrix\n",
    "3. Try the k-means++ init for the clustering algorithm\n",
    "4. Try different values for the n_init parameter\n",
    "5. Try using PCA or t-sne on your data\n",
    "6. Try using different amount of features when computing the TF-IDF representations\n",
    "7. Try using the stopwords provided with the dataset during the TF-IDF vectorization (nltk_data/corpora/reuters/stopwords)\n",
    "9. Try using a different vectorization procedure\n",
    "10. Try normalizing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research on the problem at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the realistic scenario\n",
    "\n",
    "Consider an actual project where you are given a set of documents collected from the web, that do not have any human annotations. You are tasked to group those documents semantically.\n",
    "\n",
    "Choose one of the models developed in the previous phase.\n",
    "\n",
    "1. Look through the cluster texts in order to get a sense of what each cluster represents.\n",
    "2. For each cluster extract the most important/characteristic words and see what they point to\n",
    "    - you can do this by evaluating documents that are closer to the center of the cluster\n",
    "    - you can do this by running statistics on the entire cluster\n",
    "    - you can use different measures for the importance of a word (number of distinct documents within which the word appears, tfidf for the documents closer to the center, etc)\n",
    "    \n",
    "Use this exercise in order to get a sense of the data and to get a sense of how the clustering algorithm grouped them and what those groups represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretating results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue the investigation of our problem using a different clustering procedure, namely agglomerative clustering. For that we will use the implemenation provided by the scipy library (https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Repeat evaluating the different setups presented in the previous set of exercises in order to investigate the current clustering method. In addition:\n",
    "\n",
    "1. Evaluate different linking methods\n",
    "2. Evaluate different metrics (such as cosine)\n",
    "\n",
    "And see how they impact the performance and learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research on the problem at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "Finally, we will make use of the DBSCAN algorithm in order to cluster the data (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Investigate the DBSCAN model. In addition to the previous setups:\n",
    "\n",
    "1. Evaluate the distribution of distances between samples in the training data and use them as a reference point when deciding the parameters\n",
    "2. Perform a grid search on the parameters\n",
    "3. Use a different association rule for evaluation, which allows multiple clusters to pe assigned to a single class, for instance, based on the confusion matrix, assign each cluster to the class with which it has the most samples in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research on the problem at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreting results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
